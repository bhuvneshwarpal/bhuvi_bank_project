# -*- coding: utf-8 -*-
"""banking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GosN4N0dQ6ku21nkcEgkRPceIMMHJ0dm

## **Project Title : Predicting the effectiveness of bank marketing campaigns**

### **Problem Description**
## The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. The classification goal is to predict if the client will subscribe a term deposit (variable y).

# **Data Description**
# **Input Variables**
# **Bank Client data**:
## age (numeric)
## job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')
## marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)
## education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')
## default: has credit in default? (categorical: 'no','yes','unknown')
## housing: has housing loan? (categorical: 'no','yes','unknown')
# loan: has personal loan? (categorical: 'no','yes','unknown')
## **Related with the last contact of the current campaign:**
## contact: contact communication type (categorical: 'cellular','telephone')
## month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')
## day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')
# duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.
# Other attributes:
## campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
## pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
## previous: number of contacts performed before this campaign and for this client (numeric)
## poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')
# Output variable (desired target):
## y - has the client subscribed a term deposit? (binary: 'yes','no')

# Loading Libraries & Data
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import random
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
# %matplotlib inline

from prettytable import PrettyTable
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, KFold, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, log_loss, classification_report, precision_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from imblearn.under_sampling import NearMiss
from collections import Counter
from xgboost import XGBClassifier
from imblearn.over_sampling import RandomOverSampler
from imblearn.combine import SMOTETomek
from sklearn.neighbors import KNeighborsClassifier

import warnings
warnings.filterwarnings('ignore')

#mounting drive
from google.colab import drive
drive.mount('/content/drive')

#Loading Dataset
df= pd.read_csv('/content/drive/MyDrive/Almabetter/bank/bank-full.csv',delimiter=';')

"""#  Analyzing Dataset"""

df.head()

df.tail()

#Shape of the Dataset
df.shape

"""# Information About Dataset"""

df.info()

"""## **Checking Null Values**"""

df.isnull().sum()

"""There is no null values in the dataset

# Summary Statastics
"""

df.describe()

"""Target Variable"""

df.y.value_counts()

"""As We Can See that Data is highly imbalanced.

# List of numeric columns
"""

numerical_cols = list(df.select_dtypes(exclude=['object']))
numerical_cols

"""## **List of Categorical columns**"""

category_cols = list(df.select_dtypes(include=['object']))
category_cols

"""## **EDA On Dataset**

### **On Target Variable**
"""

#visualising how many customers have subscribed 
labels = 'Not Subscribed' , 'Subscribed'
sizes = df.y.value_counts()
colors = ['silver', 'green']
explode = (0.1, 0.0)
plt.pie(sizes, explode=explode, labels=labels, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=180)
plt.axis('equal')
plt.title("How many people have subscribed the product ?",fontsize=20)
plt.plot()
fig=plt.gcf()
fig.set_size_inches(6,6)
plt.show()

"""Only 11.7% people have subscribed to our product

## **Bar Graph Representation of Each Variable**
"""

for col in category_cols:
    plt.figure(figsize=(10,4))
    sns.barplot(df[col].value_counts().values, df[col].value_counts().index)
    plt.title(col)
    plt.tight_layout()

"""**Graph Representation of each Categorical variable with respect to Target variable**"""

sns.set(rc={'figure.figsize':(20,6)})

def bar_compare(x,y):
  sns.set_style("whitegrid")
  plt.figure(figsize=(18,6))
  sns.countplot(x=df[x],data=df,hue=df[y])
  plt.title('Count Plot of {x} for target variable Y'.format(x=x.title(),fontsize = 25))
  plt.show()

#ploting countplot for different categorical columns
for col in category_cols[:-1]:
  bar_compare(col,'y')

"""### **From the above plots we can analyze that:**
 

*  Top contacted clients are from job type: 'blue-collar', 'management' & 'technician'.
* Success rate is highest for student.
* Most of the clients contacted have previous outcome as 'unknown'.
* Most of the clients (approx 1/3 of total) are contacted in the month of May but the success rate is only 6.7%.
* March has highest success rate.
* Most of the people are contacted through cellular.
* As seen for default variable, less client are contacted who have loan.
* Very few clients are contacted who are defaulter.
* Most of the people who are contacted have tertiray or secondary education.
* As we can see that married people are more tend to invest in product and have subscribed more in comparison to others.

### **Age distribution in our dataset**
"""

fig, ax = plt.subplots()
fig.set_size_inches(11, 7)
sns.distplot(df["age"],
                hist_kws = {'color':'#DC143C',
                     'linestyle':'--', 'alpha':0.7}, bins=60);

"""Our dataset have more number of people in late 20s and early 30s

### **Pairplot Representation of each variable with respect to Target Variable**
"""

sns.pairplot(df, hue="y",diag_kind="hist")
plt.show()

"""## **Observation:**

* 
For most of the variables our pair plot is overlapping a lot.
* 
Pair plots of age-campaign and day-campaign are much efficient in distinguishing between different classes with very few overlapes.



"""

pylab.rcParams['figure.figsize'] = 12,8
sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.set_style("whitegrid")

"""## **Scatter plot distribution of age vs balance**"""

#visualising age and balance with respect to whether customer has subscribed or not
sns.scatterplot(x="age", y="balance",hue='y',data=df,x_bins=50,y_bins=50,alpha=0.9)

"""From above scatter plot we can see 40-60 age group people have subscribed the product. People having high balance have very low subscription rate.This also shows that our dataset have some balance below 0 and we can use this scatter plot to remove some outliers from our data set.

## **Scatter plot distribution of pdays vs duration**
"""

plt.figure(figsize=(15,6))
sns.scatterplot(x=df['pdays'],y=df['duration'],hue=df['y'])
plt.show()

"""As scatter plot is depicting that most of the client that had been last contacted falls in 0-400(days)

#### **Scatter plot distribution of duration vs balance**
"""

plt.figure(figsize=(15,6))
sns.scatterplot(x=df['duration'],y=df['balance'] ,hue=df['y'])
plt.show()

"""**Most of the clients who have taken a term deposit do not have very high balance(mostly in between 0-20000)**"""

#no.of calls performed in this campaign
df.groupby('y')['campaign'].mean()

"""**On average 2 calls leads to success for the clients who have taken a term deposit.**

### **Plotting education, campaign, day and sbscribed by taking 6000 random samples**
"""

sample=df.sample(6000)
sns.scatterplot(x="day", y="campaign", hue="education", data=sample,x_bins=50,y_bins=50,alpha=0.85, style="y",s=130 );

"""**This scatter plot shows that our campaign is more focused on people having secondary education. We also do more campaign on the month end. We need to focus on other education class as well also we need to be more uniform in our campaign accross all days of month.**

### **Visualising age and balance relation with respect to subscribed**
"""

sns.relplot(x="age", y="balance", kind="line", data=df, hue='y',height=6,aspect=2 );

"""**We can see that for age 20-60 people with having higher balance are the ones who subscribe more. But as we move towards higher age we see mixed distribution of subscribing term deposit.**

### **Outliers Detection On Features**

**On duration Feature**
"""

# converting call duration from seconds to minute
plt.figure(figsize=(8,6))
df['duration'] = df['duration']/60
sns.boxplot(y=df['duration'], x=df['y'])
plt.title('Box plot of duration vs y(target variable)')
plt.xlabel('y:target variable')

"""**On pdays Feature**"""

plt.figure(figsize=(8,6))
sns.boxplot(y=df['pdays'], x=df['y'])
plt.title('Box plot of pdays vs y (target variable)')
plt.xlabel('y: target variable')

"""**On previous Feature**"""

plt.figure(figsize=(8,6))
sns.boxplot(y=df['previous'], x=df['y'])
plt.title('Box plot of previous vs y(target variable)')
plt.xlabel('y:target variable')

"""**On age Feature**"""

plt.figure(figsize=(8,6))
sns.boxplot(y=df['age'], x=df['y'])
plt.title('Box plot of age vs y(target variable)')
plt.xlabel('y:target variable')

"""**As We can see that there are many Outliers in No part As well Yes Part but here our data is Imbalanced so we are keeping this Outliers**"""

data=df.copy()

"""#### **Replacing yes with 1 and No with 0**"""

data.replace(to_replace={'y':'yes'}, value=1, inplace=True)
data.replace(to_replace={'y':'no'}, value=0, inplace=True)

"""### **Converting categorical variables into numeric**"""

# Converting categorical variables into numeric

data['job'] = data['job'].astype('category').cat.codes
data['marital'] = data['marital'].astype('category').cat.codes
data['education'] = data['education'].astype('category').cat.codes
data['contact'] = data['contact'].astype('category').cat.codes
data['poutcome'] = data['poutcome'].astype('category').cat.codes
data['month'] = data['month'].astype('category').cat.codes
data['default'] = data['default'].astype('category').cat.codes
data['loan'] = data['loan'].astype('category').cat.codes
data['housing'] = data['housing'].astype('category').cat.codes

"""### **Heatmap of the dataset**"""

plt.subplots(figsize=(20,12))
sns.heatmap(data.corr().abs(), annot=True)

"""**This shows that duration and housing are highly correlated with target variable y (Y/N). Also pdays are also impacting poutcome.**

### **Splitting Dataset into Train set and Test set**
"""

X = data.drop(['y'], axis=1)
y = data['y']

#Splitting the Dataset inro Train Set and Test Set
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

print('Train data shape {} {}'.format(x_train.shape, y_train.shape))
print('Test data shape {} {}'.format(x_test.shape, y_test.shape))

"""### **Applying Decision Tree Classifier**"""

clf = DecisionTreeClassifier(class_weight='balanced', min_weight_fraction_leaf = 0.01)

clf.fit(x_train, y_train)
importances = clf.feature_importances_
feature_names = data.drop('y', axis=1).columns
indices = np.argsort(importances)

"""### **Feature Importance**"""

plt.style.use('seaborn-white')

def feature_importance_graph(indices, importances, feature_names):
    plt.figure(figsize=(8,6))
    plt.title("Feature Importance", fontsize=10)
    plt.barh(range(len(indices)), importances[indices], color='g',  align="center")
    plt.yticks(range(len(indices)), feature_names[indices], rotation='horizontal',fontsize=14)
    plt.ylim([-1, len(indices)])
    
feature_importance_graph(indices, importances, feature_names)
plt.show()

"""###**Feature Selection**

Important features we are going to consider for machine learning models:


* contact  
* poutcome 
*  month
* housing
* pdays
*age
* balance
* day
* marital
* campaign

Here, Target variable is highly influcenced by duration variable so we are not considering it.

### Selecting only those features which are important as per feature importance graph shown above
"""

data=data[['contact','poutcome','month','housing','pdays','age','balance','day','marital','campaign','y']]

data.head(2)

"""### **Splitting Dataset into training and testing**"""

X=data.drop(['y'],axis=1)
y = data['y']

#Splitting the Dataset inro Train Set and Test Set
x_train, x_test, y_train, y_test = train_test_split(data.drop(['y'], axis=1), y, test_size=0.20, random_state=42)

"""# **Models Implementation**

**Logistic Regression**

**Logistic Regression (Under sampling)**

**Random Forest (Under sampling)**

**Random Forest (Over sampling)**

**KNN (Over sampling)**

**XGBoost (Over sampling)**

## **Applying Logistic Regression**
"""

#creating hyperparameter grid for Logistic regression
log_reg_grid={'C': np.logspace(-4,4,20),
              'solver':['liblinear']}

#create hyperparameter grid for Random forest
rf_grid= {'n_estimators' : np.arange(10,1000,50),
          'max_depth' : [None, 3,5,10],
          'min_samples_split' : np.arange(2,20,2),
          'min_samples_leaf' : np.arange(1,20,2),
          "max_features": [0.5,1,"sqrt","auto"],
          }

# dictionary to store accuracy and roc score for each model
score = {}

#tune logistic regression
np.random.seed(42)
#setting up random search hyperparameter search for logistic regression
rs_log_reg = RandomizedSearchCV(LogisticRegression(class_weight='balanced'),
                                param_distributions=log_reg_grid,
                                cv=5,
                                n_iter=20,
                                scoring='roc_auc', return_train_score=True, n_jobs=-1)


# fit random search hyperparameter search for logistic regression
rs_log_reg.fit(x_train,y_train)
print('Best parameters:  {}'.format(rs_log_reg.best_params_))
print('Best score: {}'.format(rs_log_reg.best_score_))

from sklearn.metrics import log_loss

model = LogisticRegression(C=0.012742749857031334, solver='liblinear', n_jobs=-1,class_weight='balanced')
model.fit(x_train, y_train)
y_probs_train = model.predict_proba(x_train)
y_probs_test = model.predict_proba(x_test)
y_predicted_train = model.predict(x_train)
y_predicted_test = model.predict(x_test)

# keep probabilities for the positive outcome only
y_probs_train = y_probs_train[:, 1]
y_probs_test = y_probs_test[:, 1]

# calculate AUC and Accuracy
plt.figure(figsize=(9,7))
train_auc = roc_auc_score(y_train, y_probs_train)
test_auc = roc_auc_score(y_test, y_probs_test)
train_acc = accuracy_score(y_train, y_predicted_train)
test_acc = accuracy_score(y_test, y_predicted_test)
f1_s=f1_score(y_test,y_predicted_test)
p_score=precision_score(y_test,y_predicted_test)
print('*'*50)
print('Train AUC: %.3f' % train_auc)
print('Test AUC: %.3f' % test_auc)
print('*'*50)
print('Train Accuracy: %.3f' % train_acc)
print('Test Accuracy: %.3f' % test_acc)

score['Logistic Regression'] = [test_auc, test_acc,f1_s,p_score]

# calculate roc curve
train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_probs_train)
test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_probs_test)
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(train_fpr, train_tpr, marker='.', label='Train AUC')
plt.plot(test_fpr, test_tpr, marker='.', label='Test AUC')
plt.legend()
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.show()

print(classification_report(y_test,y_predicted_test))

"""### **Performing Undersampling On Dataset**"""

Yes = data[data['y']==1]
No = data[data['y']==0]

print(Yes.shape,No.shape)

# Implementing Undersampling for Handling Imbalanced 
from imblearn.over_sampling import SMOTE

sm = SMOTE(sampling_strategy='auto', random_state=42)
X_res,y_res=sm.fit_resample(X,y)

X_res.shape,y_res.shape

print('Original dataset shape {}'.format(Counter(y)))
print('Resampled dataset shape {}'.format(Counter(y_res)))

#Splitting the Dataset inro Train Set and Test Set
x_train, x_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.20, random_state=42)

"""**Scaling dataset**"""

#scaling our dataset for fit

sc_X = StandardScaler()
x_train = sc_X.fit_transform(x_train)
x_test = sc_X.transform(x_test)

"""## **Logistic Regression (Under Sampling)**"""

parameters = {'C':[(10**i)*x for i in range(-4, 1) for x in [1,3,5]]}

model = LogisticRegression(class_weight='balanced')
clf = RandomizedSearchCV(model, parameters, cv=5, scoring='roc_auc', return_train_score=True, n_jobs=-1)
clf.fit(x_train, y_train)
print('Best parameters:  {}'.format(clf.best_params_))
print('Best score: {}'.format(clf.best_score_))

from sklearn.metrics import log_loss

model = LogisticRegression(C=0.5, n_jobs=-1)
model.fit(x_train, y_train)
y_probs_train = model.predict_proba(x_train)
y_probs_test = model.predict_proba(x_test)
y_predicted_train = model.predict(x_train)
y_predicted_test = model.predict(x_test)

# keep probabilities for the positive outcome only
y_probs_train = y_probs_train[:, 1]
y_probs_test = y_probs_test[:, 1]

# calculate AUC and Accuracy
plt.figure(figsize=(9,7))
train_auc = roc_auc_score(y_train, y_probs_train)
test_auc = roc_auc_score(y_test, y_probs_test)
train_acc = accuracy_score(y_train, y_predicted_train)
test_acc = accuracy_score(y_test, y_predicted_test)
f1_s= f1_score(y_test,y_predicted_test)
p_score=precision_score(y_test,y_predicted_test)
print('*'*50)
print('Train AUC: %.3f' % train_auc)
print('Test AUC: %.3f' % test_auc)
print('*'*50)
print('Train Accuracy: %.3f' % train_acc)
print('Test Accuracy: %.3f' % test_acc)

score['Logistic Regression (Under sampling)'] = [test_auc, test_acc,f1_s,p_score]

# calculate roc curve
train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_probs_train)
test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_probs_test)
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(train_fpr, train_tpr, marker='.', label='Train AUC')
plt.plot(test_fpr, test_tpr, marker='.', label='Test AUC')
plt.legend()
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.show()

print(classification_report(y_test,y_predicted_test))

"""## **Random Forest Classifier (Under Sampling)**"""

params = {'n_estimators':[75, 100, 250, 500], 'max_depth':[3, 5, 10, 15, 25]}
model = RandomForestClassifier(class_weight='balanced', n_jobs=-1)
clf = RandomizedSearchCV(model, param_distributions=params, cv=5, scoring='roc_auc', random_state=42, n_jobs=-1, return_train_score=True)
clf.fit(x_train, y_train)
print('Best parameters:  {}'.format(clf.best_params_))
print('Best score: {}'.format(clf.best_score_))

model1 = RandomForestClassifier(n_estimators=500, max_depth=15, n_jobs=-1)
model1.fit(x_train, y_train)
y_probs_train = model1.predict_proba(x_train)
y_probs_test = model1.predict_proba(x_test)
y_predicted_train = model1.predict(x_train)
y_predicted_test = model1.predict(x_test)

# keep probabilities for the positive outcome only
y_probs_train = y_probs_train[:, 1]
y_probs_test = y_probs_test[:, 1]

# calculate AUC and Accuracy
plt.figure(figsize=(9,7))
train_auc = roc_auc_score(y_train, y_probs_train)
test_auc = roc_auc_score(y_test, y_probs_test)
train_acc = accuracy_score(y_train, y_predicted_train)
test_acc = accuracy_score(y_test, y_predicted_test)
f1_s= f1_score(y_test,y_predicted_test)
p_score=precision_score(y_test,y_predicted_test)
print('*'*50)
print('Train AUC: %.3f' % train_auc)
print('Test AUC: %.3f' % test_auc)
print('*'*50)
print('Train Accuracy: %.3f' % train_acc)
print('Test Accuracy: %.3f' % test_acc)

score['Random Forest (Under sampling)'] = [test_auc, test_acc,f1_s,p_score]

# calculate roc curve
train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_probs_train)
test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_probs_test)
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(train_fpr, train_tpr, marker='.', label='Train AUC')
plt.plot(test_fpr, test_tpr, marker='.', label='Test AUC')
plt.legend()
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.show()

print(classification_report(y_test,y_predicted_test))

"""## **Fixing our imbalanced data set with oversampling and applying a model**"""

#scaling our dataset for fit

sc_X = StandardScaler()
x_train = sc_X.fit_transform(x_train)
x_test = sc_X.transform(x_test)

"""### **1. Random Forest Classifier (Over Sampling)**"""

k_fold = KFold(n_splits=10, shuffle=True, random_state=0)

#RandomForest Classifier implmentation on our balanced dataset with few hyperparameters 
rfc = RandomForestClassifier(n_estimators = 200, n_jobs=2, random_state = 12)#criterion = entopy,gini
rfc.fit(x_train, y_train)
rfcpred = rfc.predict(x_test)
RFCCV = (cross_val_score(rfc, x_train, y_train, cv=k_fold, n_jobs=2, scoring = 'accuracy').mean())

#displaying score 
models = pd.DataFrame({
                'Models': ['Random Forest Classifier'],
                'Score':  [RFCCV]})

models.sort_values(by='Score', ascending=False)

#printing confusion matrix
print('RFC Confusion Matrix\n', confusion_matrix(y_test, rfcpred))

#printing classification report
print('RFC Reports\n',classification_report(y_test, rfcpred))

y_probs_train = rfc.predict_proba(x_train)
y_probs_test = rfc.predict_proba(x_test)
y_predicted_train = rfc.predict(x_train)
y_predicted_test = rfc.predict(x_test)

# keep probabilities for the positive outcome only
y_probs_train = y_probs_train[:, 1]
y_probs_test = y_probs_test[:, 1]

# calculate AUC and Accuracy
plt.figure(figsize=(9,7))
train_auc = roc_auc_score(y_train, y_probs_train)
test_auc = roc_auc_score(y_test, y_probs_test)
train_acc = accuracy_score(y_train, y_predicted_train)
test_acc = accuracy_score(y_test, y_predicted_test)
f1_s= f1_score(y_test,y_predicted_test)
p_score=precision_score(y_test,y_predicted_test)

print('*'*50)
print('Train AUC: %.3f' % train_auc)
print('Test AUC: %.3f' % test_auc)
print('*'*50)
print('Train Accuracy: %.3f' % train_acc)
print('Test Accuracy: %.3f' % test_acc)

score['Random Forest (Over sampling)'] = [test_auc, test_acc,f1_s,p_score]

# calculate roc curve
train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_probs_train)
test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_probs_test)
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(train_fpr, train_tpr, marker='.', label='Train AUC')
plt.plot(test_fpr, test_tpr, marker='.', label='Test AUC')
plt.legend()
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.show()

print(classification_report(y_test,y_predicted_test))

"""## **2. KNN (Over Sampling)**"""

knn = KNeighborsClassifier(n_jobs=-1)
k_range = list(range(5, 31))
param_distributions = dict(n_neighbors=k_range)
clf = RandomizedSearchCV(knn, param_distributions, cv=10, scoring='roc_auc', random_state=42, n_jobs=-1, return_train_score=True)
clf.fit(x_train, y_train)
print('Best parameters:  {}'.format(clf.best_params_))
print('Best score: {}'.format(clf.best_score_))

model = KNeighborsClassifier(n_neighbors=10, n_jobs=-1)
model.fit(x_train, y_train)
y_probs_train = model.predict_proba(x_train)
y_probs_test = model.predict_proba(x_test)
y_predicted_train = model.predict(x_train)
y_predicted_test = model.predict(x_test)

# keep probabilities for the positive outcome only
y_probs_train = y_probs_train[:, 1]
y_probs_test = y_probs_test[:, 1]

# calculate AUC and Accuracy
plt.figure(figsize=(9,7))
train_auc = roc_auc_score(y_train, y_probs_train)
test_auc = roc_auc_score(y_test, y_probs_test)
train_acc = accuracy_score(y_train, y_predicted_train)
test_acc = accuracy_score(y_test, y_predicted_test)
f1_s= f1_score(y_test,y_predicted_test)
p_score=precision_score(y_test,y_predicted_test)

print('*'*50)
print('Train AUC: %.3f' % train_auc)
print('Test AUC: %.3f' % test_auc)
print('*'*50)
print('Train Accuracy: %.3f' % train_acc)
print('Test Accuracy: %.3f' % test_acc)

score['KNN (Over sampling)'] = [test_auc, test_acc,f1_s,p_score]

# calculate roc curve
train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_probs_train)
test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_probs_test)
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(train_fpr, train_tpr, marker='.', label='Train AUC')
plt.plot(test_fpr, test_tpr, marker='.', label='Test AUC')
plt.legend()
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.show()

print(classification_report(y_test,y_predicted_test))

"""## **XGBoost Model (Over Sampling)**"""

#using XGBoost in RandomizedSearch to get best result

folds = 3
param_comb = 5
prams={
    'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],
     'n_estimators':[100,200,500,1000],
     'max_depth':[3,5,10],
    'colsample_bytree':[0.1,0.3,0.5,1],
    'subsample':[0.1,0.3,0.5,1]
}    
skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)

xgb=XGBClassifier()

#xgb.fit(X_train, y_train)
random_search = RandomizedSearchCV(xgb,param_distributions= prams ,n_iter=param_comb, scoring='roc_auc', n_jobs=4, verbose=3, random_state=1001 )
random_search.fit(x_train, y_train)
y_probs_train = random_search.predict_proba(x_train)
y_probs_test = random_search.predict_proba(x_test)
y_predicted_train = random_search.predict(x_train)
y_predicted_test = random_search.predict(x_test)

# keep probabilities for the positive outcome only
y_probs_train = y_probs_train[:, 1]
y_probs_test = y_probs_test[:, 1]

# calculate AUC and Accuracy
plt.figure(figsize=(9,7))
train_auc = roc_auc_score(y_train, y_probs_train)
test_auc = roc_auc_score(y_test, y_probs_test)
train_acc = accuracy_score(y_train, y_predicted_train)
test_acc = accuracy_score(y_test, y_predicted_test)
f1_s= f1_score(y_test,y_predicted_test)
p_score=precision_score(y_test,y_predicted_test)

print('*'*50)
print('Train AUC: %.3f' % train_auc)
print('Test AUC: %.3f' % test_auc)
print('*'*50)
print('Train Accuracy: %.3f' % train_acc)
print('Test Accuracy: %.3f' % test_acc)

score['XGBoost (Over sampling)'] = [test_auc, test_acc,f1_s,p_score]

# calculate roc curve
train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_probs_train)
test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_probs_test)
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(train_fpr, train_tpr, marker='.', label='Train AUC')
plt.plot(test_fpr, test_tpr, marker='.', label='Test AUC')
plt.legend()
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.show()

print(classification_report(y_test,y_predicted_test))

score

"""**Random Forest(Under Sampling ) Model performing good among all models so considering it for the feature importance graph**

## **Feature Importance**
"""

importances = model1.feature_importances_
feature_names = data.drop('y', axis=1).columns
indices = np.argsort(importances)

plt.style.use('seaborn-white')

def feature_importance_graph(indices, importances, feature_names):
    plt.figure(figsize=(12,6))
    plt.title("Feature Importance", fontsize=10)
    plt.barh(range(len(indices)), importances[indices], color='g',  align="center")
    plt.yticks(range(len(indices)), feature_names[indices], rotation='horizontal',fontsize=14)
    plt.ylim([-1, len(indices)])
    
feature_importance_graph(indices, importances, feature_names)
plt.show()

"""## **Conclusion**


* It was a great learning experience working on a Bank dataset.
* Our dataset consist of categorical and numerical features.
* We have 17 independent features, out of these only half of them are important.
* 'balance' is the most important feature while 'education' is the least important feature.
* Month of May have seen the highest number of clients contacted but have the least success rate. Highest success rate is observed for end month of the financial year as well as the calendar year. So one can say that our dataset have some kind of seasonality.
* Different machine learning models are trained and tested on the dataset. Out of those Random Forest and XGBoost performs best. Logistic Regression is also an important model as it results in high AUC score.
* Different models are summarized in table below.

"""

print('***************  Comparison of different models  ****************')
table = PrettyTable(['Model', 'Test AUC', 'Test Accuracy','F1_score','Precision'])
for item in score.items():
    table.add_row([item[0], item[1][0], item[1][1], item[1][2], item[1][3]])
print(table)

"""## **Random Forest and XGBoost are performing good among all the other models.**

## **Thank You**
"""